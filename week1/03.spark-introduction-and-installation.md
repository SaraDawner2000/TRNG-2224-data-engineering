# Apache Spark

an opensource, distributed processing framework designed for large-scale data processing and analytics.


#### Spark Cluster

![spark cluster](./images/spark-cluster-overview.png)


##### Driver Program

- It creates SparkContext, defines RDDs, and submits tasks.
- It is responsible for:

    - Defining the job logic (transformations & actions).
    - Creating the Directed Acyclic Graph (DAG) of execution stages.
    - Communicating with the Cluster Manager to request resources.
    - Distributing tasks to executors and tracking execution.

- Coordinates everything.

##### Cluster Manager

Allocates resources (CPU & memory) across applications.

**Spark supports:**

- Standalone (built-in)
- YARN (Hadoop)
- Mesos
- Kubernetes

##### Worker Nodes

- Machines in the cluster managed by the Cluster Manager.
- Each worker runs executors to process tasks.


##### Executor

- A JVM process launched on each worker.

**Responsible for:**

- Running tasks
- Storing RDD partitions in memory (cache)
- Reporting back to the driver


##### Tasks

- The smallest unit of work in Spark.
- A job is split into stages, and each stage is made of tasks.
- Each task is run by an executor on a partition of data.


##### Cache/Storage (RDD Caching)

- RDDs can be cached in memory (or disk) using .cache() or .persist().
- Useful for reusing the same data across multiple actions (e.g. multiple .collect() or .count() calls).

#### Spark and Pyspark installation

##### Python installation in gitpod

```sh
sudo apt-get update && sudo apt-get install -y python3 python3-pip
```

#### Java Installation and path setup

```sh
# install java 11
### Common DataFrame API methods

#### Transformations

##### Narrow Transformations

- narrow transformations process data within each partition independetly, without needing to combine data from other partitions.
- faster and more efficient because they avoid data shuffling between partitions. 

1. `select()` : selecting specific rows
2. `filter()`: Applying a filter condition to rows. 
3. `map()`: Applying a function to each row. 
4. `union()`: Combining two DataFrames with identical schemas. 
5. `withColumn()`: Adding a new column based on existing ones. 
6. `drop()`: Removing a column. 

##### Wide Transformations

- Wide transformations require data to be redistributed across partitions, often involving shuffling data based on keys.

1. `groupBy()`: Grouping data based on a column, which often requires shuffling to aggregate data from different partitions. 
2. `join()`: Joining two DataFrames, which requires shuffling data to combine rows based on a join key. 
3. `distinct()`: Removing duplicate rows, which might require shuffling to compare rows across partitions. 

#### Actions

1. `count()`: returns number of rows in a Dataframe
2. `show()`: display DataFrame content
3. `take(n)`: return first n rows from a DataFrame
4. `first()`: return first row from a DataFrame
5. `write()`: save DataFrame to storage
# verify installation
java -version

# get java path
readlink -f $(which java)

# add java path

echo 'export JAVA_HOME=/path/to/java' >> ~/.bashrc
echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc

# apply changes
source ~/.bashrc

```

[spark on ubuntu](https://phoenixnap.com/kb/install-spark-on-ubuntu)

[apache spark download](https://spark.apache.org/downloads.html)

[pyspark installation](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)


### RDDs, DataFrames and Datasets

#### Resilient Distributed Dataset (RDD)

- immutable distributed collection of elements of data, partitioned across nodes that can be operated in parallel with a low level API.
- offers transformations and actions

##### When to use RDDs?


- You need fine-grained, low-level transformations and actions on your data.
- Your data is unstructured (e.g., media streams, text streams).
- You prefer functional programming constructs over domain-specific expressions.
- You don't require schema enforcement or column-based data access.
- You're willing to trade off some performance and optimization benefits available in DataFrames/Datasets.














