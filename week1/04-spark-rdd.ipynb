{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc5d93d",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "an RDD is the fundamental data structure of Apache Spark. It's a fault-tolerant, distributed collection of elements that can be operated on in parallel.\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "- Immutable\n",
    "- Lazy evaluation\n",
    "- Fault tolerant (via lineage info)\n",
    "- Partitioned across cluster nodes\n",
    "- Can be cached in memory\n",
    "\n",
    "### SparkContext and SparkConf\n",
    "\n",
    "\n",
    "SparkContext is the entry point for Spark functionality.\n",
    "\n",
    "#### `SparkConf`\n",
    "\n",
    "- Configuration for Spark application\n",
    "\n",
    "**Common settings:**\n",
    "\n",
    "- setMaster(\"local[*]\") – Use local mode with all cores\n",
    "- setAppName(\"RDDExample\") – Application name\n",
    "\n",
    "### transformations\n",
    "\n",
    "Transformations create a new RDD from an existing one. They are lazy – not executed until an action is triggered.\n",
    "\n",
    "| Transformation  | Description                                          |\n",
    "| --------------- | ---------------------------------------------------- |\n",
    "| `map(func)`     | Returns a new RDD by applying `func` to each element |\n",
    "| `filter(func)`  | Filters elements for which `func` returns true       |\n",
    "| `flatMap(func)` | Like map but flattens the result                     |\n",
    "| `distinct()`    | Removes duplicates                                   |\n",
    "| `union(rdd)`    | Combines two RDDs                                    |\n",
    "| `groupByKey()`  | Groups values with same key                          |\n",
    "| `reduceByKey()` | Aggregates values with same key using a function     |\n",
    "| `sortBy(func)`  | Sorts RDD by computed key                            |\n",
    "\n",
    "\n",
    "### actions\n",
    "\n",
    "Actions trigger computation and return results or write data.\n",
    "\n",
    "| Action             | Description                            |\n",
    "| ------------------ | -------------------------------------- |\n",
    "| `collect()`        | Returns all elements to driver         |\n",
    "| `count()`          | Returns number of elements             |\n",
    "| `first()`          | Returns first element                  |\n",
    "| `take(n)`          | Returns first `n` elements             |\n",
    "| `reduce(func)`     | Reduces elements using binary operator |\n",
    "| `saveAsTextFile()` | Writes RDD to text files               |\n",
    "\n",
    "\n",
    "\n",
    "reference - [spark rdd docs](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80fcd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /workspace/.pyenv_mirror/user/current/lib/python3.10/site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1b247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -XX:+UseContainerSupport -XX:ActiveProcessorCount=1\n",
      "Picked up JAVA_TOOL_OPTIONS: -XX:+UseContainerSupport -XX:ActiveProcessorCount=1\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/24 18:51:42 WARN Utils: Your hostname, krishnagopi-trng2224dat-g3q9nc1wf47, resolves to a loopback address: 127.0.0.1; using 10.0.5.2 instead (on interface eth0)\n",
      "25/06/24 18:51:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/24 18:51:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# SparkContext and SparkConf\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"salesDemo\").setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd1caba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "330e73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the file into RDD\n",
    "\n",
    "sales_raw = sc.textFile(\"file:////workspace/TRNG-2224-data-engineering/week1/datasets/sales.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c49a839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert each line into a tuple (ProductID, Category, Amount)\n",
    "\n",
    "records = sales_raw.map(lambda x: x.split(\",\")).map(lambda x: (int(x[0]), x[1], int(x[2])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d9677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a (Category, Amount) RDD\n",
    "\n",
    "category_sales = records.map(lambda x: (x[1], x[2]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22401f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Total sales by category\n",
    "\n",
    "total_sales_by_category = category_sales.reduceByKey(lambda x,y: x+y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f63568bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furniture', 538.4090909090909),\n",
       " ('Clothing', 443.0),\n",
       " ('Electronics', 547.4166666666666),\n",
       " ('Books', 407.3636363636364)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Average sale per category\n",
    "\n",
    "average_sales_by_category = category_sales.mapValues(lambda x: (x, 1)).reduceByKey(lambda x,y: (x[0] + y[0], x[1]+ y[1])).mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "average_sales_by_category.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96717e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1014, 'Electronics', 987)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Highest transaction\n",
    "\n",
    "max_tran = records.max(key=lambda x : x[2])\n",
    "\n",
    "max_tran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e032282f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Electronics', 13138)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highest selling category\n",
    "\n",
    "highest_selling_category = total_sales_by_category.max(key=lambda x: x[1] )\n",
    "\n",
    "highest_selling_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Furniture', 11845), ('Electronics', 13138), ('Toys', 11794)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Categories with sales above 5000\n",
    "\n",
    "high_selling_cat_5k = total_sales_by_category.filter(lambda x: x[1] > 5000)\n",
    "\n",
    "high_selling_cat_5k.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "231d2cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sales by cat\n",
      "[('Furniture', 11845), ('Clothing', 9303), ('Electronics', 13138), ('Books', 4481), ('Toys', 11794)]\n",
      "Avg sales per cat\n",
      "[('Furniture', 538.4090909090909), ('Clothing', 443.0), ('Electronics', 547.4166666666666), ('Books', 407.3636363636364), ('Toys', 536.0909090909091)]\n"
     ]
    }
   ],
   "source": [
    "# print final results\n",
    "\n",
    "print(\"Total sales by cat\")\n",
    "print(total_sales_by_category.collect())\n",
    "\n",
    "print(\"Avg sales per cat\")\n",
    "\n",
    "print(average_sales_by_category.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65a088",
   "metadata": {},
   "source": [
    "**Assignment:**\n",
    "\n",
    "1. find all product IDs where the amount is greater than 900.\n",
    "2. Find all transactions that belong to the “Furniture” category.\n",
    "3. Count how many transactions belong to the “Electronics” category.\n",
    "4. Find average amount for each category.\n",
    "5. Find the highest amount and the corresponding product ID.\n",
    "6. Find the total number of unique categories.\n",
    "7. For each category, find the product ID with the highest sale.\n",
    "8. Count how many products were sold for less than 300.\n",
    "9. Sort the transactions by amount in descending order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3292be",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
