{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accad24f",
   "metadata": {},
   "source": [
    "# Spark DataFrames API\n",
    "\n",
    "\n",
    "### Dataframe\n",
    "\n",
    "- DataFrames are ditributed collections of records, all with pre-defined structure(schema - structure and data types of all columns)\n",
    "-  DataFrames are built on Spark's core concepts but with structure, optimization and SQL-like operations for data manipulation.\n",
    "- DataFrames track their schema and provide native support for many common SQL functions and relational operators\n",
    "- DataFrames are evaluated as DAGs, using lazy evaluation and providing lineage and fault tolerance.\n",
    "- DataFrames are immutable\n",
    "\n",
    "### SparkContext vs SparkConfig\n",
    "\n",
    "- SparkSession is Spark application entry point. \n",
    "- Introduced in spark 2.0 as a unified entry point for all contexts (formerly instantiated individually as SparkContext, SQLContext, HiveContext, StreamingContext)\n",
    "\n",
    "<i>Note: In databricks it is automatically created for you as spark</i>\n",
    "\n",
    "### DataFrame API Optimizations\n",
    "\n",
    "- **Adaptive Query Execution:** Dynamic plan adjustments during runtime based on actual data characteristics and execution patterns.\n",
    "- **In-Memory Columnar Storage(Tungsten):** In-Memory coloumnar format for all the DataFrames enabling efficient analytical query performance and reduced memory footprint.\n",
    "- **Built-in Statistics** - Automatic statistics collection when saving to optimized formats (Parqurt, Delta in databricks) enables smarter query planning and execution.\n",
    "- **Catalyst Optimizer:** Query optimization engine that coverts DataFrame operations into an optimized execution plan\n",
    "\n",
    "\n",
    "<i>**Note** Databricks comes with a native vectorized query engine that accelerates query execution using photon engine</i>\n",
    "\n",
    "**DataFrame Query Planning:** \n",
    "\n",
    "- When a DataFrame is evaluated, the driver creates an optimized execution plan through a series of transformations \n",
    "- Converts the logical plan into phycal execution that minimizes resource usage and execution time. (Unresolved LP -> analysed LP -> optimized LP -> Physical Plan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9e173",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60216694",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating DataFrames - DataFrameReader\n",
    "# supports multiple formats such as JSON, CSV, Parquet, ORC, Text or Binary files, existing RDD, and an external db\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c52631",
   "metadata": {},
   "source": [
    "### DataFrame Data Types\n",
    "\n",
    "#### Primitive\n",
    "\n",
    "**`pyspark.sql.types.DataType`**\n",
    "\n",
    "- `ByteType`\n",
    "- `ShortType`\n",
    "- `IntegerType`\n",
    "- `LongType`\n",
    "- `FloatType`\n",
    "- `DoubleType`\n",
    "- `BooleanType`\n",
    "- `StringType`\n",
    "- `BinaryType`\n",
    "- `TimestampType`\n",
    "- `DateType`\n",
    "\n",
    "#### complex data types\n",
    "\n",
    "- `ArrayType`\n",
    "- `MapType`\n",
    "- `StructType`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd435e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DataFrame Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79dc794",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# custom schema definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1dc9d9",
   "metadata": {},
   "source": [
    "### Common DataFrame API methods\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "##### Narrow Transformations\n",
    "\n",
    "- narrow transformations process data within each partition independetly, without needing to combine data from other partitions.\n",
    "- faster and more efficient because they avoid data shuffling between partitions. \n",
    "\n",
    "1. `select()` : selecting specific rows\n",
    "2. `filter()`: Applying a filter condition to rows. \n",
    "3. `map()`: Applying a function to each row. \n",
    "4. `union()`: Combining two DataFrames with identical schemas. \n",
    "5. `withColumn()`: Adding a new column based on existing ones. \n",
    "6. `drop()`: Removing a column. \n",
    "\n",
    "##### Wide Transformations\n",
    "\n",
    "- Wide transformations require data to be redistributed across partitions, often involving shuffling data based on keys.\n",
    "\n",
    "1. `groupBy()`: Grouping data based on a column, which often requires shuffling to aggregate data from different partitions. \n",
    "2. `join()`: Joining two DataFrames, which requires shuffling data to combine rows based on a join key. \n",
    "3. `distinct()`: Removing duplicate rows, which might require shuffling to compare rows across partitions. \n",
    "\n",
    "#### Actions\n",
    "\n",
    "1. `count()`: returns number of rows in a Dataframe\n",
    "2. `show()`: display DataFrame content\n",
    "3. `take(n)`: return first n rows from a DataFrame\n",
    "4. `first()`: return first row from a DataFrame\n",
    "5. `write()`: save DataFrame to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95a59e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Map, Shuffle and Reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b211c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e0a9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Filter Active Customers Over 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46152d95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Group by Country and Get Average Spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47462d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add a New Column for Spend Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f327d67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# DataFrameWriter - flexible output formats and partitioning. supports various save modes (overwrite, append)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
