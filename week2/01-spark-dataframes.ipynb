{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accad24f",
   "metadata": {},
   "source": [
    "# Spark DataFrames API\n",
    "\n",
    "\n",
    "### Dataframe\n",
    "\n",
    "- DataFrames are ditributed collections of records, all with pre-defined structure(schema - structure and data types of all columns)\n",
    "-  DataFrames are built on Spark's core concepts but with structure, optimization and SQL-like operations for data manipulation.\n",
    "- DataFrames track their schema and provide native support for many common SQL functions and relational operators\n",
    "- DataFrames are evaluated as DAGs, using lazy evaluation and providing lineage and fault tolerance.\n",
    "- DataFrames are immutable\n",
    "\n",
    "### SparkContext vs SparkSession\n",
    "\n",
    "- SparkSession is Spark application entry point. \n",
    "- Introduced in spark 2.0 as a unified entry point for all contexts (formerly instantiated individually as SparkContext, SQLContext, HiveContext, StreamingContext)\n",
    "\n",
    "<i>Note: In databricks it is automatically created for you as spark</i>\n",
    "\n",
    "### DataFrame API Optimizations\n",
    "\n",
    "- **Adaptive Query Execution:** Dynamic plan adjustments during runtime based on actual data characteristics and execution patterns.\n",
    "- **In-Memory Columnar Storage(Tungsten):** In-Memory coloumnar format for all the DataFrames enabling efficient analytical query performance and reduced memory footprint.\n",
    "- **Built-in Statistics** - Automatic statistics collection when saving to optimized formats (Parqurt, Delta in databricks) enables smarter query planning and execution.\n",
    "- **Catalyst Optimizer:** Query optimization engine that coverts DataFrame operations into an optimized execution plan\n",
    "\n",
    "\n",
    "<i>**Note** Databricks comes with a native vectorized query engine that accelerates query execution using photon engine</i>\n",
    "\n",
    "**DataFrame Query Planning:** \n",
    "\n",
    "- When a DataFrame is evaluated, the driver creates an optimized execution plan through a series of transformations \n",
    "- Converts the logical plan into phycal execution that minimizes resource usage and execution time. (Unresolved LP -> analysed LP -> optimized LP -> Physical Plan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e9e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -XX:+UseContainerSupport -XX:ActiveProcessorCount=1\n",
      "Picked up JAVA_TOOL_OPTIONS: -XX:+UseContainerSupport -XX:ActiveProcessorCount=1\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/30 18:13:56 WARN Utils: Your hostname, krishnagopi-trng2224dat-g3q9nc1wf47, resolves to a loopback address: 127.0.0.1; using 10.0.5.2 instead (on interface eth0)\n",
      "25/06/30 18:13:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/30 18:13:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CutomerDFExample\").getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60216694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+\n",
      "|         customer_id|           name|               email|age|gender|             country|signup_date|         last_login|is_active|total_spent|\n",
      "+--------------------+---------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+\n",
      "|20780d38-901f-450...| Michael Malone|    dhart@haynes.com| 58|  Male|    Saint Barthelemy| 2021-04-29|2024-10-20 15:56:26|     true|     3733.6|\n",
      "|a2c56b05-acdc-4a7...|     Edwin Wall| bradley08@yahoo.com| 33|  Male|United Arab Emirates| 2025-01-02|2025-06-19 22:44:59|     true|    3708.71|\n",
      "|2fe8ff2e-19ea-493...|  Rachel Strong|heather15@schmidt...| 61| Other|              Israel| 2023-02-13|2025-04-12 21:14:26|     true|    2993.41|\n",
      "|5fd9f4a6-2134-41b...|Eddie Rodriguez|mitchell49@hotmai...| 20|  Male|             Nigeria| 2024-07-06|2025-03-06 17:09:20|     true|    1171.33|\n",
      "+--------------------+---------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrames - DataFrameReader\n",
    "# supports multiple formats such as JSON, CSV, Parquet, ORC, Text or Binary files, existing RDD, and an external db\n",
    "\n",
    "\n",
    "df_customers = spark.read.csv(\"file:///workspace/TRNG-2224-data-engineering/week2/datasets/customer_data.csv\", header= True, inferSchema=True)\n",
    "\n",
    "df_customers.show(4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c52631",
   "metadata": {},
   "source": [
    "### DataFrame Data Types\n",
    "\n",
    "#### Primitive\n",
    "\n",
    "**`pyspark.sql.types.DataType`**\n",
    "\n",
    "- `ByteType`\n",
    "- `ShortType`\n",
    "- `IntegerType`\n",
    "- `LongType`\n",
    "- `FloatType`\n",
    "- `DoubleType`\n",
    "- `BooleanType`\n",
    "- `StringType`\n",
    "- `BinaryType`\n",
    "- `TimestampType`\n",
    "- `DateType`\n",
    "\n",
    "#### complex data types\n",
    "\n",
    "- `ArrayType`\n",
    "- `MapType`\n",
    "- `StructType`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd435e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('customer_id', StringType(), True), StructField('name', StringType(), True), StructField('email', StringType(), True), StructField('age', IntegerType(), True), StructField('gender', StringType(), True), StructField('country', StringType(), True), StructField('signup_date', DateType(), True), StructField('last_login', TimestampType(), True), StructField('is_active', BooleanType(), True), StructField('total_spent', DoubleType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame Schema\n",
    "\n",
    "df_customers.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d79dc794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      " |-- last_login: timestamp (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- total_spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# custom schema definition\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, BooleanType, TimestampType, DateType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True ),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), True),\n",
    "    StructField(\"last_login\", TimestampType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), True),\n",
    "    StructField(\"total_spent\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_customers = spark.read.csv(\"file:///workspace/TRNG-2224-data-engineering/week2/datasets/customer_data.csv\", header= True, schema=custom_schema)\n",
    "\n",
    "df_customers.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d95d7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      " |-- last_login: timestamp (nullable = true)\n",
      " |-- is_active: boolean (nullable = true)\n",
      " |-- total_spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DDL Schema\n",
    "\n",
    "ddl_schema = \"\"\"\n",
    "    customer_id STRING,\n",
    "    name STRING,\n",
    "    email STRING,\n",
    "    age INT,\n",
    "    gender STRING,\n",
    "    country STRING,\n",
    "    signup_date DATE,\n",
    "    last_login TIMESTAMP,\n",
    "    is_active BOOLEAN,\n",
    "    total_spent DOUBLE\n",
    "\"\"\"\n",
    "\n",
    "df_customers = spark.read.csv(\"file:///workspace/TRNG-2224-data-engineering/week2/datasets/customer_data.csv\", header= True, schema=ddl_schema)\n",
    "\n",
    "df_customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1dc9d9",
   "metadata": {},
   "source": [
    "### Common DataFrame API methods\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "##### Narrow Transformations\n",
    "\n",
    "- narrow transformations process data within each partition independetly, without needing to combine data from other partitions.\n",
    "- faster and more efficient because they avoid data shuffling between partitions. \n",
    "\n",
    "1. `select()` : selecting specific rows\n",
    "2. `filter()`: Applying a filter condition to rows. \n",
    "3. `map()`: Applying a function to each row. \n",
    "4. `union()`: Combining two DataFrames with identical schemas. \n",
    "5. `withColumn()`: Adding a new column based on existing ones. \n",
    "6. `drop()`: Removing a column. \n",
    "\n",
    "##### Wide Transformations\n",
    "\n",
    "- Wide transformations require data to be redistributed across partitions, often involving shuffling data based on keys.\n",
    "\n",
    "1. `groupBy()`: Grouping data based on a column, which often requires shuffling to aggregate data from different partitions. \n",
    "2. `join()`: Joining two DataFrames, which requires shuffling data to combine rows based on a join key. \n",
    "3. `distinct()`: Removing duplicate rows, which might require shuffling to compare rows across partitions. \n",
    "\n",
    "#### Actions\n",
    "\n",
    "1. `count()`: returns number of rows in a Dataframe\n",
    "2. `show()`: display DataFrame content\n",
    "3. `take(n)`: return first n rows from a DataFrame\n",
    "4. `first()`: return first row from a DataFrame\n",
    "5. `write()`: save DataFrame to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f95a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|          country|           revenue|\n",
      "+-----------------+------------------+\n",
      "|            Macao|            3022.4|\n",
      "|            Yemen|           1407.35|\n",
      "|         Kiribati|           1833.03|\n",
      "|           Guyana|           1149.76|\n",
      "|           Jersey|           1398.78|\n",
      "|   Norfolk Island|           2929.03|\n",
      "|         Djibouti|           4291.06|\n",
      "|            Tonga|           2167.22|\n",
      "|           Malawi|           4418.94|\n",
      "|          Germany|2965.7599999999998|\n",
      "|           Jordan|           2175.54|\n",
      "|            Sudan|            800.07|\n",
      "|           Greece|           2734.41|\n",
      "|             Togo|           3097.22|\n",
      "|          Ecuador|           3340.19|\n",
      "|            Qatar|           5966.42|\n",
      "|          Lesotho|            1977.1|\n",
      "|       Madagascar|           3372.53|\n",
      "|Brunei Darussalam|           4201.99|\n",
      "|             Peru|           2080.31|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Map, Shuffle and Reduce\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "\n",
    "df_customers.filter(df_customers.age > 30) \\\n",
    "                    .select(\"country\", \"total_spent\")\\\n",
    "                    .groupBy(\"country\") \\\n",
    "                    .agg(sum(\"total_spent\").alias(\"revenue\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d9b211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+--------------------+\n",
      "|           name|               email|             country|\n",
      "+---------------+--------------------+--------------------+\n",
      "| Michael Malone|    dhart@haynes.com|    Saint Barthelemy|\n",
      "|     Edwin Wall| bradley08@yahoo.com|United Arab Emirates|\n",
      "|  Rachel Strong|heather15@schmidt...|              Israel|\n",
      "|Eddie Rodriguez|mitchell49@hotmai...|             Nigeria|\n",
      "+---------------+--------------------+--------------------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "# Select Specific Columns\n",
    "\n",
    "df_customers.select(\"name\", \"email\", \"country\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a93e0a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+\n",
      "|         customer_id|               name|               email|age|gender|             country|signup_date|         last_login|is_active|total_spent|\n",
      "+--------------------+-------------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+\n",
      "|20780d38-901f-450...|     Michael Malone|    dhart@haynes.com| 58|  Male|    Saint Barthelemy| 2021-04-29|2024-10-20 15:56:26|     true|     3733.6|\n",
      "|a2c56b05-acdc-4a7...|         Edwin Wall| bradley08@yahoo.com| 33|  Male|United Arab Emirates| 2025-01-02|2025-06-19 22:44:59|     true|    3708.71|\n",
      "|2fe8ff2e-19ea-493...|      Rachel Strong|heather15@schmidt...| 61| Other|              Israel| 2023-02-13|2025-04-12 21:14:26|     true|    2993.41|\n",
      "|b290eed5-e70c-48d...|       Kayla Powell|johnnash@hotmail.com| 50| Other|        Burkina Faso| 2021-02-23|2025-01-13 19:44:54|     true|     2850.1|\n",
      "|3a86653f-6238-47a...|    Nicolas Kennedy|catherineblack@wa...| 55|Female|   Brunei Darussalam| 2020-07-14|2025-01-01 09:16:51|     true|    3818.16|\n",
      "|635e0da3-f233-44b...|          Amber Ray|   alan33@taylor.net| 56|Female|               Aruba| 2024-02-28|2024-12-26 15:55:41|     true|    3675.69|\n",
      "|0553e350-b76a-4e6...|   Christopher Hall|arthurolson@yahoo...| 63|  Male|             Ecuador| 2022-08-15|2024-10-10 13:16:35|     true|    3340.19|\n",
      "|ce723df7-7023-41a...|         Tara Brady|qmcdonald@hotmail...| 67|Female|            Djibouti| 2021-06-10|2025-05-04 19:21:29|     true|    4291.06|\n",
      "|bf645baf-2bde-4a1...|     Cheryl Griffin|parksjames@graves...| 60|Female|             Croatia| 2023-06-20|2025-06-18 08:09:35|     true|    3084.74|\n",
      "|4cf1af58-ab8a-464...|          Alex Moon|rodriguezmichael@...| 42|Female|Central African R...| 2024-06-01|2024-12-10 06:13:24|     true|    3454.61|\n",
      "|a242ee65-a9a3-42d...|    Rebecca Mcmahon|smithzachary@hotm...| 67|  Male|            Honduras| 2024-06-25|2024-09-03 10:53:21|     true|    4126.83|\n",
      "|02af714d-88d6-475...|     Jimmy Franklin|  qbeasley@gmail.com| 53|  Male|               Aruba| 2025-02-25|2024-08-14 21:48:09|     true|    3440.38|\n",
      "|c00024a4-b3ae-4d0...|   Richard Stephens|   james47@gmail.com| 66|  Male|         Puerto Rico| 2022-09-23|2025-01-23 15:37:31|     true|    4365.54|\n",
      "|d950098c-cd38-480...|        Karen Brown|   rgordon@glass.com| 37| Other|              Serbia| 2022-05-16|2024-08-09 20:35:27|     true|     848.91|\n",
      "|bea6fbf6-c6ca-460...|       Amy Thompson|jenkinspaul@yahoo...| 66|  Male|           Gibraltar| 2022-08-29|2024-11-23 04:08:32|     true|    3034.83|\n",
      "|8ec68e5a-02f7-434...|Mrs. Karen Jones MD|   cindy50@yahoo.com| 49|  Male|           Gibraltar| 2025-06-09|2025-06-16 23:42:37|     true|    4652.58|\n",
      "|4ffab53d-a05b-4e3...|         Alan Jones|  taylor51@blair.com| 69|Female|   Brunei Darussalam| 2020-07-23|2024-08-13 10:37:15|     true|     383.83|\n",
      "|c6bb99c7-fb56-42d...|        Ryan Turner|  andrew49@gmail.com| 54|  Male|               Congo| 2025-05-07|2025-04-19 18:36:08|     true|    3686.35|\n",
      "|4c490337-6133-49d...|  Christopher Huang|xhendricks@morris...| 70|  Male|       Cote d'Ivoire| 2023-09-07|2024-12-07 13:15:50|     true|     729.12|\n",
      "|a36a99c7-5573-4c6...|    Heather Stewart|martin06@hotmail.com| 48| Other|              Jersey| 2020-07-22|2025-06-26 23:37:30|     true|    1398.78|\n",
      "+--------------------+-------------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Filter Active Customers Over 30\n",
    "\n",
    "df_customers.filter((df_customers.age>30) & (df_customers.is_active == True)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46152d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|          country|  avg(total_spent)|\n",
      "+-----------------+------------------+\n",
      "|            Macao|            3022.4|\n",
      "|            Yemen|           1407.35|\n",
      "|         Kiribati|           1833.03|\n",
      "|           Guyana|           1149.76|\n",
      "|           Jersey|           1398.78|\n",
      "|   Norfolk Island|           2929.03|\n",
      "|         Djibouti|           4291.06|\n",
      "|            Tonga|           2167.22|\n",
      "|           Malawi|           4418.94|\n",
      "|          Germany|1482.8799999999999|\n",
      "|           Jordan|           2175.54|\n",
      "|     Saint Helena|           4639.94|\n",
      "|            Sudan|            800.07|\n",
      "|           Greece|           2734.41|\n",
      "|             Togo|           3097.22|\n",
      "|Equatorial Guinea|           4459.59|\n",
      "|          Ecuador|           3340.19|\n",
      "|            Qatar|           2983.21|\n",
      "|          Lesotho|            988.55|\n",
      "|       Madagascar|           3372.53|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Group by Country and Get Average Spend\n",
    "\n",
    "df_customers.groupBy(\"country\").avg(\"total_spent\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce47462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+--------------+\n",
      "|            name|               email|total_spent|spend_category|\n",
      "+----------------+--------------------+-----------+--------------+\n",
      "|  Michael Malone|    dhart@haynes.com|     3733.6|          High|\n",
      "|      Edwin Wall| bradley08@yahoo.com|    3708.71|          High|\n",
      "|   Rachel Strong|heather15@schmidt...|    2993.41|        Medium|\n",
      "| Eddie Rodriguez|mitchell49@hotmai...|    1171.33|        Medium|\n",
      "|    Kayla Powell|johnnash@hotmail.com|     2850.1|        Medium|\n",
      "| Kathleen Nelson|     qpena@gmail.com|    1180.16|        Medium|\n",
      "| Nicolas Kennedy|catherineblack@wa...|    3818.16|          High|\n",
      "| Jacqueline Reid|smithjoshua@baker...|    1767.23|        Medium|\n",
      "|  Matthew Mendez|jeremymontoya@har...|     600.83|           Low|\n",
      "|      Jay Little|brogers@wright-wo...|    4252.72|          High|\n",
      "|       Amber Ray|   alan33@taylor.net|    3675.69|          High|\n",
      "| Elizabeth Ellis|jjohnson@smith-th...|     486.12|           Low|\n",
      "|      Mary Brown|bernardsarah@gmai...|    2929.03|        Medium|\n",
      "|Christopher Hall|arthurolson@yahoo...|    3340.19|          High|\n",
      "|      Tara Brady|qmcdonald@hotmail...|    4291.06|          High|\n",
      "|   William Moore|griffindakota@hot...|    2321.71|        Medium|\n",
      "|   Laura Mathews|dylanrivera@hicks...|    1840.83|        Medium|\n",
      "|  Cheryl Griffin|parksjames@graves...|    3084.74|          High|\n",
      "|   Annette Moore|ewood@ford-santan...|     900.67|           Low|\n",
      "|       Alex Moon|rodriguezmichael@...|    3454.61|          High|\n",
      "+----------------+--------------------+-----------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Add a New Column for Spend Category\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_customer_with_catgory = df_customers.withColumn(\n",
    "    \"spend_category\",\n",
    "    when(df_customers.total_spent >3000, \"High\")\n",
    "    .when(df_customers.total_spent> 1000, \"Medium\")\n",
    "    .otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "df_customer_with_catgory.select(\"name\", \"email\",\"total_spent\" ,\"spend_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f327d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# DataFrameWriter - flexible output formats and partitioning. supports various save modes (overwrite, append)\n",
    "\n",
    "df_customer_with_catgory.write.mode(\"overwrite\").parquet(\"cutomer_oputput.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
