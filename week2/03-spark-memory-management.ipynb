{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043af201",
   "metadata": {},
   "source": [
    "#### Spark Memory Management\n",
    "\n",
    "\n",
    "![spark-memory](/workspace/TRNG-2224-data-engineering/week2/images/spark-execution-memory.png)\n",
    "\n",
    "\n",
    "In Apache Spark, understanding memory management is essential to optimizing performance. Spark has a unified memory management model (since Spark 1.6+), which governs how memory is divided and used by different components. Here's a breakdown of the key memory types:\n",
    "\n",
    "\n",
    "###  1. **Reserved Memory**\n",
    "\n",
    "* A small portion of JVM heap reserved and not usable by Spark.\n",
    "* **Default size:** 300MB (configurable via `spark.memory.storage.reserved` but not usually changed).\n",
    "* Reserved for internal metadata, JVM tasks, and to prevent OOM errors.\n",
    "\n",
    "\n",
    "\n",
    "###  2. **On-Heap Memory**\n",
    "\n",
    "*  Memory within the JVM heap, used by Spark when `spark.memory.offHeap.enabled = false`.\n",
    "* **Total size:** Determined by `spark.executor.memory`.\n",
    "* **Used for:**\n",
    "\n",
    "  * Execution (shuffle, joins, aggregations, sorts)\n",
    "  * Storage (caching/persisted RDDs or DataFrames)\n",
    "  * User memory (custom objects, UDFs, broadcast vars)\n",
    "\n",
    "\n",
    "### 3. **Off-Heap Memory**\n",
    "\n",
    "*  Memory outside JVM heap, accessed using unsafe APIs.\n",
    "* **Enabled via:** `spark.memory.offHeap.enabled = true`\n",
    "* **Size set by:** `spark.memory.offHeap.size`\n",
    "* **Use cases:**\n",
    "\n",
    "  * Tungsten’s binary data storage\n",
    "  * External shuffle\n",
    "  * More efficient, less GC pressure\n",
    "\n",
    "\n",
    "### 4. **Unified Memory (Spark 1.6+)**\n",
    "\n",
    "Spark divides usable memory (excluding reserved) into:\n",
    "\n",
    "```\n",
    "spark.executor.memory - reservedMemory\n",
    "  └──→ unifiedMemory = execution + storage\n",
    "```\n",
    "\n",
    "#### a. **Execution Memory**\n",
    "\n",
    "* For tasks like joins, aggregations, sorts, and shuffles.\n",
    "* **Dynamic:** Can borrow from storage if needed and available.\n",
    "* **Evicts:** Cached blocks only when absolutely necessary.\n",
    "\n",
    "####  b. **Storage Memory**\n",
    "\n",
    "* To store cached or persisted RDD/DataFrame blocks and broadcast variables.\n",
    "* **Eviction policy:** Least recently used (LRU).\n",
    "* **Dynamic:** Can borrow from execution memory, but only if execution is not actively using it.\n",
    "\n",
    "##### Storage & Execution share memory — dynamic allocation helps better memory utilization.\n",
    "\n",
    "\n",
    "###  5. **User Memory**\n",
    "\n",
    "*  \\~25% of `spark.executor.memory`, not governed by unified memory manager.\n",
    "* **Used for:**\n",
    "\n",
    "  * Custom data structures\n",
    "  * UDF intermediate states\n",
    "  * Broadcast variables (partial)\n",
    "  * Spark internal bookkeeping\n",
    "* **Not tunable directly**, but indirectly via reducing UDF usage or tuning executor memory.\n",
    "\n",
    "\n",
    "### 6. **Overhead Memory**\n",
    "\n",
    "*  Memory for non-JVM needs like YARN/Mesos container overhead, native libraries, Python/R processes (if using PySpark or SparkR).\n",
    "* **Configurable via:**\n",
    "\n",
    "  * `spark.yarn.executor.memoryOverhead`\n",
    "  * `spark.executor.memoryOverhead`\n",
    "* **Default:** max(384MB, 0.10 \\* spark.executor.memory)\n",
    "\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Memory Type | Purpose                               | Where?         | Configurable?         |\n",
    "| ----------- | ------------------------------------- | -------------- | --------------------- |\n",
    "| Reserved    | JVM internals                         | On-heap        | No (hardcoded default) |\n",
    "| Execution   | Shuffles, joins, aggregations         | Unified memory | Yes                     |\n",
    "| Storage     | Cached/persisted RDDs, broadcast vars | Unified memory | Yes                    |\n",
    "| User        | UDFs, custom objects                  | On-heap        | No (implicit)          |\n",
    "| Off-heap    | External shuffle, Tungsten binary     | Off-heap       | Yes                    |\n",
    "| Overhead    | Native code, containers, Python procs | Off-heap       | Yes                    |\n",
    "\n",
    "\n",
    "\n",
    "#### Caching data\n",
    "\n",
    "- Caching is the process of storing intermediate results (DataFrames/RDDs) in memory to avoid recomputation in future actions.\n",
    "- Spark evaluates lazily, so without caching, each action triggers full recomputation of the DAG.\n",
    "\n",
    "##### Where Is Data Stored When Cached?\n",
    "\n",
    "- Primary location: In Storage Memory (part of Unified Memory).\n",
    "- Fallback: If not enough memory, data is spilled to disk (depends on storage level).\n",
    "- Optional: Can store off-heap, serialized, or disk-only via persist().\n",
    "\n",
    "\n",
    "\n",
    "**`.cache()`**\n",
    "\n",
    "```py\n",
    "df.cache()\n",
    "```\n",
    "\n",
    "- Shortcut for: `.persist(StorageLevel.MEMORY_AND_DISK)`\n",
    "- Caches data in memory, spills to disk if memory is full.\n",
    "- Common and safe default for general use.\n",
    "\n",
    "**`.persist(storageLevel)`**\n",
    "\n",
    "```\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "```\n",
    "\n",
    "- Gives control over how and where data is stored.\n",
    "\n",
    "##### StorageLevel Option\n",
    "\n",
    "(As of Spark 3.4)\n",
    "\n",
    "- DISK_ONLY: CPU efficient, memory efficient, slow to access, data is serialized when stored on disk\n",
    "- DISK_ONLY_2: disk only, replicated 2x\n",
    "- DISK_ONLY_3: disk only, replicated 3x\n",
    "- MEMORY_AND_DISK: spills to disk if there's no space in memory\n",
    "- MEMORY_AND_DISK_2: memory and disk, replicated 2x\n",
    "- MEMORY_AND_DISK_DESER(default): same as MEMORY_AND_DISK, deserialized in both for fast access\n",
    "- MEMORY_ONLY: CPU efficient, memory intensive\n",
    "- MEMORY_ONLY_2: memory only, replicated 2x - for resilience, if one executor fails\n",
    "\n",
    "\n",
    "\n",
    "- SER is CPU intensive, memory saving as data is compact while DESER is CPU efficient, memory intensive\n",
    "- Size of data on disk is lesser as data is in serialized format, while deserialized in memory as JVM objects for faster access\n",
    "\n",
    "**When to use what?**\n",
    "\n",
    "| Storage Level          | Space Used | CPU Time | In Memory | On Disk | Serialized |\n",
    "| ---------------------- | ---------- | -------- | --------- | ------- | ---------- |\n",
    "| MEMORY\\_ONLY           | High       | Low      | Yes       | No      | No         |\n",
    "| MEMORY\\_ONLY\\_SER      | Low        | High     | Yes       | No      | Yes        |\n",
    "| MEMORY\\_AND\\_DISK      | High       | Medium   | Some      | Some    | Some       |\n",
    "| MEMORY\\_AND\\_DISK\\_SER | Low        | High     | Some      | Some    | Yes        |\n",
    "| DISK\\_ONLY             | Low        | High     | No        | Yes     | Yes        |  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f396861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
