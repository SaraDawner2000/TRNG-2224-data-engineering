{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbec0803",
   "metadata": {},
   "source": [
    "# Spark Optimization\n",
    "\n",
    "### Partitioning\n",
    "\n",
    "- Partitioning refers to dividing data into logical chunks (partitions) across nodes.\n",
    "- Effective partitioning improves parallelism, reduces shuffle, and enhances query performance.\n",
    "\n",
    "#### Partitioning in memory\n",
    "\n",
    "- Repartition\n",
    "    - allows to specify the desired number of partitions and the columns to partition by\n",
    "    - shuffles the data to create the specified number of partitions\n",
    "\n",
    "- Coalesce\n",
    "    - reduces the number of partitions by merging them\n",
    "    - useful when you want to decrease the number of partitions for efficiency\n",
    "\n",
    "#### Partitioning on disk\n",
    "\n",
    "\n",
    "- `partitionBy()` method is used to partition the data into a file system, resulting in multiple sub-directories.\n",
    "- this enhances the read performance for downstream systems.\n",
    "- This function can be applied to one or multiple column values while writing a DataFrame to the disk.\n",
    "\n",
    "\n",
    "### Bucketing\n",
    "\n",
    "- Bucketing organizes data into fixed number of buckets using the hash of a column.\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces shuffle during joins and aggregations.\n",
    "- Supports efficient bucketed joins and sort-merge joins.\n",
    "\n",
    "[spark performace tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be29eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CustomersOrdersExample\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d55268d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+----------+------+\n",
      "|            order_id|         customer_id|order_date|product_id|amount|\n",
      "+--------------------+--------------------+----------+----------+------+\n",
      "|02a777e0-5571-42c...|0e99a07c-c7a5-43d...|2023-04-21|     P1031|375.94|\n",
      "|1c5a3e4d-f8de-47b...|3a69ac3e-6726-431...|2021-09-25|     P1086|373.51|\n",
      "|a5b65d4d-3ac0-45d...|3a69ac3e-6726-431...|2024-01-04|     P1054| 61.73|\n",
      "|b752df2c-aa68-41e...|c63cab5f-dc06-484...|2024-01-16|     P1029| 64.97|\n",
      "+--------------------+--------------------+----------+----------+------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "orders_df = spark.read.csv(\"file:///workspace/TRNG-2224-data-engineering/week2/datasets/orders.csv\", inferSchema=True, header=True)\n",
    "\n",
    "orders_df.show(4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01795c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+--------------+\n",
      "|         customer_id|           name|               email|age|gender|             country|signup_date|         last_login|is_active|total_spent|spend_category|\n",
      "+--------------------+---------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+--------------+\n",
      "|20780d38-901f-450...| Michael Malone|    dhart@haynes.com| 58|  Male|    Saint Barthelemy| 2021-04-29|2024-10-20 15:56:26|     true|     3733.6|          High|\n",
      "|a2c56b05-acdc-4a7...|     Edwin Wall| bradley08@yahoo.com| 33|  Male|United Arab Emirates| 2025-01-02|2025-06-19 22:44:59|     true|    3708.71|          High|\n",
      "|2fe8ff2e-19ea-493...|  Rachel Strong|heather15@schmidt...| 61| Other|              Israel| 2023-02-13|2025-04-12 21:14:26|     true|    2993.41|        Medium|\n",
      "|5fd9f4a6-2134-41b...|Eddie Rodriguez|mitchell49@hotmai...| 20|  Male|             Nigeria| 2024-07-06|2025-03-06 17:09:20|     true|    1171.33|        Medium|\n",
      "+--------------------+---------------+--------------------+---+------+--------------------+-----------+-------------------+---------+-----------+--------------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "customers_df = spark.read.parquet(\"file:///workspace/TRNG-2224-data-engineering/week2/cutomer_oputput.parquet\")\n",
    "\n",
    "customers_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dd2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning  in memory - repartition\n",
    "\n",
    "\n",
    "customers_df.repartition(4, \"country\").write.mode(\"overwrite\").parquet(\"customers_partitioned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b85c7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_customer_df = customers_df.repartitionByRange(4, \"country\").sortWithinPartitions(\"total_spent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f068945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------------------+-----------+\n",
      "|partition_id|country                                     |total_spent|\n",
      "+------------+--------------------------------------------+-----------+\n",
      "|0           |Cape Verde                                  |115.9      |\n",
      "|0           |Brazil                                      |378.71     |\n",
      "|0           |Brunei Darussalam                           |383.83     |\n",
      "|0           |Cameroon                                    |435.75     |\n",
      "|0           |Chile                                       |562.2      |\n",
      "|0           |Bhutan                                      |900.67     |\n",
      "|0           |Australia                                   |1097.63    |\n",
      "|0           |Burundi                                     |1129.38    |\n",
      "|0           |Cayman Islands                              |1269.56    |\n",
      "|0           |Congo                                       |1411.93    |\n",
      "|0           |Armenia                                     |1496.44    |\n",
      "|0           |Barbados                                    |1767.23    |\n",
      "|0           |Australia                                   |2345.6     |\n",
      "|0           |Cook Islands                                |2848.44    |\n",
      "|0           |Burkina Faso                                |2850.1     |\n",
      "|0           |Aruba                                       |3440.38    |\n",
      "|0           |Central African Republic                    |3454.61    |\n",
      "|0           |Aruba                                       |3675.69    |\n",
      "|0           |Congo                                       |3686.35    |\n",
      "|0           |Bahamas                                     |3732.03    |\n",
      "|0           |Bermuda                                     |3760.37    |\n",
      "|0           |Brunei Darussalam                           |3818.16    |\n",
      "|0           |Bosnia and Herzegovina                      |4163.81    |\n",
      "|0           |Azerbaijan                                  |4639.2     |\n",
      "|0           |Antarctica (the territory South of 60 deg S)|4700.76    |\n",
      "|1           |Gambia                                      |347.88     |\n",
      "|1           |Germany                                     |486.12     |\n",
      "|1           |India                                       |545.46     |\n",
      "|1           |Cote d'Ivoire                               |729.12     |\n",
      "|1           |Greenland                                   |1038.79    |\n",
      "|1           |Guyana                                      |1149.76    |\n",
      "|1           |Ethiopia                                    |1840.83    |\n",
      "|1           |Gibraltar                                   |2340.4     |\n",
      "|1           |Germany                                     |2479.64    |\n",
      "|1           |Faroe Islands                               |2692.66    |\n",
      "|1           |Greece                                      |2734.41    |\n",
      "|1           |French Guiana                               |2773.39    |\n",
      "|1           |Guatemala                                   |2982.71    |\n",
      "|1           |Gibraltar                                   |3034.83    |\n",
      "|1           |Croatia                                     |3084.74    |\n",
      "|1           |Gabon                                       |3174.49    |\n",
      "|1           |Ecuador                                     |3340.19    |\n",
      "|1           |Greenland                                   |3399.49    |\n",
      "|1           |Faroe Islands                               |3593.45    |\n",
      "|1           |Gabon                                       |3933.69    |\n",
      "|1           |Honduras                                    |4126.83    |\n",
      "|1           |Djibouti                                    |4291.06    |\n",
      "|1           |Hungary                                     |4384.98    |\n",
      "|1           |Equatorial Guinea                           |4459.59    |\n",
      "|1           |Gibraltar                                   |4652.58    |\n",
      "+------------+--------------------------------------------+-----------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "partitioned_customer_df.withColumn(\"partition_id\", spark_partition_id())\\\n",
    "    .select(\"partition_id\", \"country\", \"total_spent\") \\\n",
    "        .orderBy(\"partition_id\", \"total_spent\") \\\n",
    "        .show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13e440b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning  in memory - coalese\n",
    "\n",
    "customers_df.coalesce(1).write.mode(\"overwrite\").parquet(\"customers_coalesce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37ebef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# partitioning on Disk\n",
    "\n",
    "customers_df.write.mode(\"overwrite\").partitionBy(\"country\").parquet(\"customers_by_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34da99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter '`=`('country, Germany)\n",
      "+- Relation [customer_id#226,name#227,email#228,age#229,gender#230,signup_date#231,last_login#232,is_active#233,total_spent#234,spend_category#235,country#236] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: string, name: string, email: string, age: int, gender: string, signup_date: date, last_login: timestamp, is_active: boolean, total_spent: double, spend_category: string, country: string\n",
      "Filter (country#236 = Germany)\n",
      "+- Relation [customer_id#226,name#227,email#228,age#229,gender#230,signup_date#231,last_login#232,is_active#233,total_spent#234,spend_category#235,country#236] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(country#236) AND (country#236 = Germany))\n",
      "+- Relation [customer_id#226,name#227,email#228,age#229,gender#230,signup_date#231,last_login#232,is_active#233,total_spent#234,spend_category#235,country#236] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [customer_id#226,name#227,email#228,age#229,gender#230,signup_date#231,last_login#232,is_active#233,total_spent#234,spend_category#235,country#236] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspace/TRNG-2224-data-engineering/week2/customers_by_country], PartitionFilters: [isnotnull(country#236), (country#236 = Germany)], PushedFilters: [], ReadSchema: struct<customer_id:string,name:string,email:string,age:int,gender:string,signup_date:date,last_lo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_partitioned = spark.read.parquet(\"file:///workspace/TRNG-2224-data-engineering/week2/customers_by_country\")\n",
    "\n",
    "df_partitioned.filter(col(\"country\") == \"Germany\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "854bb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing join with buketing\n",
    "\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS bucketed_customers\")\n",
    "\n",
    "customers_df.write.bucketBy(8, \"customer_id\") \\\n",
    "    .sortBy(\"age\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"bucketed_customers\")\n",
    "\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS bucketed_orders\")\n",
    "\n",
    "orders_df.write.bucketBy(8, \"customer_id\") \\\n",
    "    .sortBy(\"order_date\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"bucketed_orders\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4aed76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [customer_id])\n",
      ":- SubqueryAlias spark_catalog.default.bucketed_orders\n",
      ":  +- Relation spark_catalog.default.bucketed_orders[order_id#254,customer_id#255,order_date#256,product_id#257,amount#258] parquet\n",
      "+- SubqueryAlias spark_catalog.default.bucketed_customers\n",
      "   +- Relation spark_catalog.default.bucketed_customers[customer_id#243,name#244,email#245,age#246,gender#247,country#248,signup_date#249,last_login#250,is_active#251,total_spent#252,spend_category#253] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: string, order_id: string, order_date: date, product_id: string, amount: double, name: string, email: string, age: int, gender: string, country: string, signup_date: date, last_login: timestamp, is_active: boolean, total_spent: double, spend_category: string\n",
      "Project [customer_id#255, order_id#254, order_date#256, product_id#257, amount#258, name#244, email#245, age#246, gender#247, country#248, signup_date#249, last_login#250, is_active#251, total_spent#252, spend_category#253]\n",
      "+- Join Inner, (customer_id#255 = customer_id#243)\n",
      "   :- SubqueryAlias spark_catalog.default.bucketed_orders\n",
      "   :  +- Relation spark_catalog.default.bucketed_orders[order_id#254,customer_id#255,order_date#256,product_id#257,amount#258] parquet\n",
      "   +- SubqueryAlias spark_catalog.default.bucketed_customers\n",
      "      +- Relation spark_catalog.default.bucketed_customers[customer_id#243,name#244,email#245,age#246,gender#247,country#248,signup_date#249,last_login#250,is_active#251,total_spent#252,spend_category#253] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [customer_id#255, order_id#254, order_date#256, product_id#257, amount#258, name#244, email#245, age#246, gender#247, country#248, signup_date#249, last_login#250, is_active#251, total_spent#252, spend_category#253]\n",
      "+- Join Inner, (customer_id#255 = customer_id#243)\n",
      "   :- Filter isnotnull(customer_id#255)\n",
      "   :  +- Relation spark_catalog.default.bucketed_orders[order_id#254,customer_id#255,order_date#256,product_id#257,amount#258] parquet\n",
      "   +- Filter isnotnull(customer_id#243)\n",
      "      +- Relation spark_catalog.default.bucketed_customers[customer_id#243,name#244,email#245,age#246,gender#247,country#248,signup_date#249,last_login#250,is_active#251,total_spent#252,spend_category#253] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [customer_id#255, order_id#254, order_date#256, product_id#257, amount#258, name#244, email#245, age#246, gender#247, country#248, signup_date#249, last_login#250, is_active#251, total_spent#252, spend_category#253]\n",
      "   +- BroadcastHashJoin [customer_id#255], [customer_id#243], Inner, BuildLeft, false\n",
      "      :- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=696]\n",
      "      :  +- Filter isnotnull(customer_id#255)\n",
      "      :     +- FileScan parquet spark_catalog.default.bucketed_orders[order_id#254,customer_id#255,order_date#256,product_id#257,amount#258] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(customer_id#255)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspace/TRNG-2224-data-engineering/week2/spark-warehouse/bucke..., PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<order_id:string,customer_id:string,order_date:date,product_id:string,amount:double>\n",
      "      +- Filter isnotnull(customer_id#243)\n",
      "         +- FileScan parquet spark_catalog.default.bucketed_customers[customer_id#243,name#244,email#245,age#246,gender#247,country#248,signup_date#249,last_login#250,is_active#251,total_spent#252,spend_category#253] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(customer_id#243)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/workspace/TRNG-2224-data-engineering/week2/spark-warehouse/bucke..., PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:string,name:string,email:string,age:int,gender:string,country:string,signup_da...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed_customers = spark.table(\"bucketed_customers\")\n",
    "bucketed_orders = spark.table(\"bucketed_orders\")\n",
    "\n",
    "\n",
    "bucketed_orders.join(bucketed_customers, \"customer_id\").explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
